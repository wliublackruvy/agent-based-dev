import sys
import os
import json
import subprocess
from pathlib import Path
from termcolor import colored

# === è·¯å¾„é…ç½® ===
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, "..")) 
sys.path.append(current_dir)

from lib.llm import call_llm, parse_json_response

PROJECT_ROOT = Path(os.getcwd())
TASKS_FILE = DOCS_DIR = PROJECT_ROOT / "docs" / "tasks.json"
PROMPT_FILE = PROJECT_ROOT / ".agents/prompts/reviewer_prompt.md"

def get_file_tree(root_path):
    """è·å–é¡¹ç›®æ–‡ä»¶ç»“æ„ï¼Œæ’é™¤æ— å…³æ–‡ä»¶å¤¹"""
    file_list = []
    for root, dirs, files in os.walk(root_path):
        # æ’é™¤é¡¹
        dirs[:] = [d for d in dirs if d not in {".git", "venv", "__pycache__", ".agents", "node_modules", ".pytest_cache"}]
        for f in files:
            if f.endswith((".py", ".ts", ".tsx", ".js", ".md", ".json")):
                rel_path = os.path.relpath(os.path.join(root, f), root_path)
                file_list.append(rel_path)
    return "\n".join(file_list)

def load_text(path):
    return path.read_text(encoding="utf-8") if path.exists() else ""

def run_pytest(test_files):
    """è¿è¡Œæµ‹è¯•"""
    print(colored(f"ğŸ§ª Running tests: {test_files}", "cyan"))
    cmd = ["pytest"] + test_files + ["-v", "--disable-warnings"]
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        return result.returncode == 0, result.stdout + result.stderr
    except Exception as e:
        return False, str(e)

def main():
    print("ğŸ•µï¸  Reviewer Agent (with De-duplication Check) is starting...")

    if not TASKS_FILE.exists(): return

    with open(TASKS_FILE, "r") as f:
        tasks = json.load(f)

    # 1. æ‰¾åˆ° Review ä»»åŠ¡
    review_task = None
    for t in tasks:
        if t.get("status") == "review":
            review_task = t
            break
    
    if not review_task:
        print("ğŸ’¤ Nothing to review.")
        return

    print(colored(f"ğŸ” Inspecting: {review_task['title']}", "blue"))
    files_str = review_task.get("file_path", "")
    files = [f.strip() for f in files_str.split(",")] if files_str else []

    # 2. å‡†å¤‡æ•°æ®ï¼šè¯»å–æ–°ç”Ÿæˆçš„ä»£ç å†…å®¹
    new_code_content = ""
    for f_path in files:
        full_path = PROJECT_ROOT / f_path
        if full_path.exists():
            new_code_content += f"\n### FILE: {f_path}\n"
            new_code_content += full_path.read_text(encoding="utf-8")
        else:
            print(colored(f"âŒ Missing file: {f_path}", "red"))
            review_task["status"] = "todo" # æ–‡ä»¶éƒ½ä¸å…¨ï¼Œç›´æ¥æ‰“å›
            return

    # 3. é˜¶æ®µä¸€ï¼šSemantic Review (æŸ¥é‡/æŸ¥å¤ç”¨)
    print(colored("ğŸ§  Performing Semantic Analysis (Duplication Check)...", "yellow"))
    
    prompt_template = load_text(PROMPT_FILE)
    file_tree = get_file_tree(PROJECT_ROOT)
    
    user_prompt = f"""
    === PROJECT FILE TREE ===
    {file_tree}

    === NEW CODE SUBMITTED ===
    {new_code_content}
    """
    
    # è°ƒç”¨ LLM åˆ¤æ–­
    llm_resp = call_llm(prompt_template, user_prompt, json_mode=True)
    review_result = parse_json_response(llm_resp)

    if not review_result:
        print(colored("âŒ Reviewer Brain Malfunction (JSON Error). Skipping...", "red"))
        # è¿™ç§æƒ…å†µä¸‹ä½ å¯ä»¥é€‰æ‹©è·³è¿‡æˆ–è€…æ‰“å›ï¼Œä¸ºäº†å®‰å…¨æˆ‘ä»¬å…ˆæ‰“å›
        return 

    if review_result.get("status") == "FAIL":
        print(colored("â›” Review Failed (Semantic Issues):", "red"))
        print(colored(f"Reason: {review_result.get('reason')}", "red"))
        
        # æ‰“å› Coder é‡åš
        review_task["status"] = "todo"
        # è¿™é‡Œçš„ feedback ä»¥åå¯ä»¥ä¼ ç»™ Coder Prompt
        review_task["feedback"] = f"Reviewer Rejected: {review_result.get('reason')}"
        
        with open(TASKS_FILE, "w", encoding="utf-8") as f:
            json.dump(tasks, f, indent=2, ensure_ascii=False)
        return
    
    print(colored("âœ… Semantic Check Passed! No obvious duplicates.", "green"))

    # 4. é˜¶æ®µäºŒï¼šExecution Review (è¿è¡Œæµ‹è¯•)
    test_files = [f for f in files if "test" in f.lower() or f.endswith("_test.py")]
    
    if test_files:
        passed, log = run_pytest(test_files)
        if passed:
            print(colored("âœ… All Tests Passed!", "green"))
            review_task["status"] = "done"
            review_task["feedback"] = "Good job."
        else:
            print(colored("âŒ Tests Failed!", "red"))
            print(log[-500:])
            review_task["status"] = "todo"
            review_task["feedback"] = f"Tests failed: {log[-500:]}"
    else:
        print(colored("âš ï¸  No tests found. Proceeding with caution.", "yellow"))
        review_task["status"] = "done"

    # ä¿å­˜æœ€ç»ˆçŠ¶æ€
    with open(TASKS_FILE, "w", encoding="utf-8") as f:
        json.dump(tasks, f, indent=2, ensure_ascii=False)
    
    print(f"Task status updated to: {review_task['status']}")

if __name__ == "__main__":
    main()