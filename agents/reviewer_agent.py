import sys
import os
import re
import argparse
import subprocess
from pathlib import Path
from termcolor import colored

# === è·¯å¾„é…ç½® ===
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(current_dir, "..")) 
sys.path.append(current_dir)

from lib.llm import call_llm, parse_json_response
from config import DOCS_DIR

PROJECT_ROOT = Path(os.getcwd())
PROMPT_FILE = PROJECT_ROOT / "agents/prompts/reviewer_prompt.md"
TASK_FILES = {
    "be": PROJECT_ROOT / "docs/TASKS_BE.md",
    "fe": PROJECT_ROOT / "docs/TASKS_FE.md"
}

def load_text(path):
    return path.read_text(encoding="utf-8") if path.exists() else ""

def save_file(path, content):
    path.write_text(content, encoding="utf-8")

def get_file_tree(root_path):
    """è·å–é¡¹ç›®æ–‡ä»¶ç»“æ„ï¼Œæ’é™¤æ— å…³æ–‡ä»¶å¤¹"""
    file_list = []
    for root, dirs, files in os.walk(root_path):
        dirs[:] = [d for d in dirs if d not in {".git", "venv", "__pycache__", ".agents", "node_modules", ".pytest_cache", "target", ".idea", ".vscode"}]
        for f in files:
            if f.endswith((".py", ".ts", ".tsx", ".js", ".md", ".json", ".java", ".xml", ".vue", ".yml", ".sql")):
                rel_path = os.path.relpath(os.path.join(root, f), root_path)
                file_list.append(rel_path)
    return "\n".join(file_list)

def find_review_task(role):
    """ä» Markdown æ–‡ä»¶ä¸­æŸ¥æ‰¾çŠ¶æ€ä¸º [review] çš„ä»»åŠ¡"""
    file_path = TASK_FILES.get(role)
    if not file_path or not file_path.exists():
        print(colored(f"âŒ Task file not found for role: {role}", "red"))
        return None

    content = load_text(file_path)
    # åŒ¹é…æ ¼å¼: - [review] Task-ID: Title | Detail | Ref: ... | Feedback: ...
    pattern = r"- \[(review)\] ((Task-\w+-\d+): (.*?) \| (.*?) \| Ref: (.*?) \| Feedback: (.*))"
    match = re.search(pattern, content)
    
    if match:
        status, full_line, task_id, title, detail, ref, feedback = match.groups()
        return {
            "id": task_id,
            "title": title,
            "detail": detail,
            "ref": ref,
            "feedback": feedback,
            "role": role,
            "raw_line": f"- [{status}] {full_line}"
        }
    return None

def update_task_status(role, task_id, new_status, feedback):
    """æ›´æ–° Markdown æ–‡ä»¶ä¸­çš„ä»»åŠ¡çŠ¶æ€å’Œåé¦ˆ"""
    file_path = TASK_FILES.get(role)
    if not file_path: return

    content = load_text(file_path)
    lines = content.splitlines()
    new_lines = []
    updated = False

    for line in lines:
        if task_id in line:
            # æ›¿æ¢çŠ¶æ€
            line = re.sub(r"\[.*?\]", f"[{new_status}]", line, count=1)
            # æ›¿æ¢åé¦ˆ (Feedback: åé¢çš„æ‰€æœ‰å†…å®¹ç›´åˆ°è¡Œå°¾)
            # æ³¨æ„ï¼šå¦‚æœåé¦ˆåŒ…å«æ¢è¡Œç¬¦ï¼ŒMarkdown è¡¨æ ¼/åˆ—è¡¨é€šå¸¸è¦æ±‚å•è¡Œï¼Œè¿™é‡Œå‡è®¾å•è¡Œ
            clean_feedback = feedback.replace("\n", " ").replace("\r", "")
            if "Feedback:" in line:
                line = re.sub(r"Feedback:.*", f"Feedback: {clean_feedback}", line)
            else:
                line += f" | Feedback: {clean_feedback}"
            updated = True
            print(colored(f"ğŸ“ Updated {task_id} -> [{new_status}]", "cyan"))
        new_lines.append(line)
    
    if updated:
        save_file(file_path, "\n".join(new_lines))

def run_tests_polyglot(role):
    """
    è¿è¡Œæµ‹è¯•
    BE -> Maven Verify
    FE -> npm run test
    """
    print(colored(f"ğŸ§ª Executing Tests for Role: {role.upper()}", "cyan"))
    
    cmd = []
    if role == "be":
        # è¿è¡Œé›†æˆæµ‹è¯•ï¼Œè·³è¿‡å•å…ƒæµ‹è¯•ä»¥åŠ å¿«é€Ÿåº¦ï¼Œæˆ–è€…å…¨è·‘
        cmd = ["mvn", "verify", "-B"] 
    elif role == "fe":
        cmd = ["npm", "run", "test"]
    else:
        return False, "Unknown role"

    try:
        # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=300, 
            cwd=PROJECT_ROOT 
        )
        return result.returncode == 0, result.stdout + result.stderr
    except Exception as e:
        return False, str(e)

def main():
    parser = argparse.ArgumentParser(description="Reviewer Agent")
    parser.add_argument("-r", "--role", choices=["be", "fe"], required=True, help="Role (be/fe)")
    parser.add_argument("-t", "--task", help="Specific Task ID (optional, defaults to first [review] task)")
    args = parser.parse_args()

    print("ğŸ•µï¸  Reviewer Agent starting...")

    # 1. æŸ¥æ‰¾ä»»åŠ¡
    task = find_review_task(args.role)
    if not task:
        print(colored("ğŸ’¤ No tasks waiting for review.", "yellow"))
        return

    if args.task and task["id"] != args.task:
        print(colored(f"âš ï¸  Found {task['id']} but you requested {args.task}. Proceeding with found task.", "yellow"))

    print(colored(f"ğŸ” Inspecting: {task['id']} - {task['title']}", "blue"))

    # 2. è·å–ä»£ç ä¸Šä¸‹æ–‡ (Code Snapshot)
    # Reviewer éœ€è¦çœ‹æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶ã€‚ç”±äº Git Diff æ¯”è¾ƒå¤æ‚ï¼Œ
    # æˆ‘ä»¬è¿™é‡Œç®€å•è¯»å–æ–‡ä»¶æ ‘ï¼Œå¹¶è®© LLM ç»“åˆé¡¹ç›®ç»“æ„è¿›è¡Œâ€˜ç›²å®¡â€™æˆ–è€…å…¨é‡å®¡æŸ¥
    # æ›´å¥½çš„åšæ³•æ˜¯ Coder åº”è¯¥æŠŠä¿®æ”¹çš„æ–‡ä»¶è·¯å¾„è®°å½•ä¸‹æ¥ï¼Œä½†ç›®å‰æ¶æ„ç®€å•ï¼Œ
    # æˆ‘ä»¬è®© LLM å®¡æŸ¥æ‰€æœ‰ç›¸å…³ä»£ç ï¼Œæˆ–è€…æˆ‘ä»¬å‡è®¾æœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶æ˜¯æœ€é‡è¦çš„ã€‚
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬è¯»å– src ä¸‹çš„æ ¸å¿ƒä»£ç ä¼ ç»™ LLMã€‚
    
    # ä¼˜åŒ–ï¼šåªè¯»å– src ç›®å½•ä¸‹çš„ä»£ç ï¼Œé¿å…å¤ªé•¿
    src_dir = PROJECT_ROOT / "src"
    code_content = ""
    file_count = 0
    MAX_CHARS = 50000 # é™åˆ¶ Token
    
    for root, _, files in os.walk(src_dir):
        for f in files:
            if f.endswith((".java", ".ts", ".vue", ".xml")):
                p = Path(os.path.join(root, f))
                # ç®€å•è¿‡æ»¤ï¼šåªçœ‹æœ€è¿‘ä¿®æ”¹çš„ï¼Ÿæˆ–è€…çœ‹å…¨éƒ¨ã€‚
                # è¿™é‡Œæš‚æ—¶è¯»å–å…¨éƒ¨ï¼Œç›´åˆ° Token ä¸Šé™
                txt = load_text(p)
                if len(code_content) + len(txt) < MAX_CHARS:
                    code_content += f"\n### FILE: {os.path.relpath(p, PROJECT_ROOT)}\n{txt}\n"
                    file_count += 1
                else:
                    break
    
    print(colored(f"ğŸ“¦ Loaded {file_count} source files for Semantic Review.", "magenta"))

    # 3. é˜¶æ®µä¸€ï¼šè¯­ä¹‰å®¡æŸ¥ (Semantic Review)
    print(colored("ğŸ§  Performing Semantic Analysis...", "yellow"))
    
    prompt_template = load_text(PROMPT_FILE)
    file_tree = get_file_tree(PROJECT_ROOT)
    
    user_prompt = f"""
    === PROJECT FILE TREE ===
    {file_tree}

    === CURRENT CODEBASE CONTENT (Sample) ===
    {code_content}
    
    Task ID: {task['id']}
    Task Description: {task['title']} - {task['detail']}
    """
    
    try:
        # JSON æ¨¡å¼è°ƒç”¨
        llm_resp = call_llm(prompt_template, user_prompt, json_mode=True)
        review_result = parse_json_response(llm_resp)
        
        if review_result and review_result.get("status") == "FAIL":
            reason = review_result.get('reason', 'Unknown semantic issue')
            print(colored("â›” Review Failed (Semantic Issues):", "red"))
            print(colored(f"Reason: {reason}", "red"))
            update_task_status(args.role, task["id"], "todo", f"Reviewer Rejected: {reason}")
            return
            
    except Exception as e:
        print(colored(f"âš ï¸ LLM Review skipped/error: {e}", "yellow"))

    print(colored("âœ… Semantic Check Passed.", "green"))

    # 4. é˜¶æ®µäºŒï¼šæ‰§è¡Œæµ‹è¯• (Execution Review)
    passed, log = run_tests_polyglot(args.role)
    
    if passed:
        print(colored("âœ… All Tests Passed!", "green"))
        update_task_status(args.role, task["id"], "done", "Passed Review & Tests")
    else:
        print(colored("âŒ Tests Failed!", "red"))
        # æå–å…³é”®é”™è¯¯æ—¥å¿—
        core_error = log[-2000:] # å–æœ€å2000å­—ç¬¦
        print(core_error)
        update_task_status(args.role, task["id"], "todo", f"Test Failed. Log: {core_error[:200]}...")

if __name__ == "__main__":
    main()